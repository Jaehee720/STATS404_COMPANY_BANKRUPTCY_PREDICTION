{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard libraries\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third-party libraries\n",
    "import boto3\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(df):\n",
    "    \"\"\"\n",
    "    Subtract output features to get X features\n",
    "    :param df: dataframe\n",
    "    :return: X columns( except the first column)\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    return df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(df):\n",
    "    \"\"\"\n",
    "    Subtract \"Bankrupt?\" column to get y feature\n",
    "    :param df: dataframe\n",
    "    :return: y column( \"Bankrupt\")\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    return df[\"Bankrupt?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(y_val, pred):\n",
    "    \"\"\"\n",
    "    Generate a confusion matrix to check the result of a prediction\n",
    "    :param y_val: Actual Y values\n",
    "    :param pred: Predicted Y values\n",
    "    :return: A confusion matrix\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    print(\n",
    "        pd.DataFrame(\n",
    "            confusion_matrix(y_val, pred),\n",
    "            columns=[\"Predicted Nagative\", \"Predicted Positive\"],\n",
    "            index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_model_pred_prob_threshold_37(X_data):\n",
    "    \"\"\"\n",
    "    Change a threshold to 0.37 to predict bankruptcy better\n",
    "    :param X_data: Actual X values\n",
    "    :return: 1 or 0 (if the probability is higher than 0.37, returns 1)\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    return np.where(cat_model.predict_proba(X_data)[:, 1] > 0.37, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bucket_name_objects():\n",
    "    \"\"\"\n",
    "    Print bucket name and object list to check bucket name and objects\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    for bucket in s3.buckets.all():\n",
    "        if bucket.name == BUCKET_NAME:\n",
    "            LOGGER.info(f\"    {bucket.name}\")\n",
    "    for file in s3.Bucket(BUCKET_NAME).objects.all():\n",
    "        LOGGER.info(f\"    {file.key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_precision_recall_chart(Xtest, ytest):\n",
    "    \"\"\"\n",
    "    Generate precision recall chart to decide threshold.\n",
    "    :param Xtest: Actual X values\n",
    "    :param ytest: Actual y value\n",
    "    :return: precision recall chart\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    probs = cat_model.predict_proba(Xtest)\n",
    "    posit_probs = probs[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(ytest, posit_probs)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "    plt.plot(thresholds, precision[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recall[:-1], \"r--\", label=\"Recall\")\n",
    "    plt.ylabel(\"Precision, Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model_to_aws(s3_fs, cat_model):\n",
    "    \"\"\"\n",
    "    Upload a model to AWS and print objects to upload a model\n",
    "    :param s3_fs: s3fs.S3FileSystem(anon=False)\n",
    "    :param cat_model: a prediction model\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    with s3_fs.open(f\"{BUCKET_NAME}/{KEY_NAME_MODEL}\", \"wb\") as file:\n",
    "        joblib.dump(cat_model, file)\n",
    "    for file in s3.Bucket(BUCKET_NAME).objects.all():\n",
    "        LOGGER.info(f\"    {file.key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_with_bankrupt_graph(importances, index, COLUMNS):\n",
    "    \"\"\"\n",
    "    Generate a correlation graph to check correlations with target feature\n",
    "    :param importances: correlations with target feautre\n",
    "    :param index: index for the order of importances\n",
    "    :param COLUMNS: column names\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.title(\"correlation with Bankrupt\")\n",
    "    plt.barh(range(len(index)), importances[index], color=\"g\", align=\"center\")\n",
    "    plt.yticks(range(len(index)), [COLUMNS[i] for i in index])\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables_high_correlation_with_bank(importances, index, COLUMNS):\n",
    "    \"\"\"\n",
    "    Save the highly correlated input features to select importat features\n",
    "    :param importances: correlations with target feautre\n",
    "    :param index: index for the order of importances\n",
    "    :param COLUMNS: column names\n",
    "    :return: high_corr_columns\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    high_corr_columns = []\n",
    "    for i in range(0, len(index)):\n",
    "        if np.abs(importances[i]) > 0.10:\n",
    "            high_corr_columns.append(COLUMNS[i])\n",
    "            print(COLUMNS[i])\n",
    "    print(len(high_corr_columns))\n",
    "    return high_corr_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dependent_variables(X, high_corr_columns):\n",
    "    \"\"\"\n",
    "    Save independent features to remove dependent features\n",
    "    :param X: X values\n",
    "    :param high_corr_columns: highly correlated features with target\n",
    "    :return: selected_columns\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    corr = X.corr()\n",
    "    high_corr_columns_bool = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= 0.9:\n",
    "                if high_corr_columns_bool[j]:\n",
    "                    high_corr_columns_bool[j] = False\n",
    "    selected_columns = X.columns[high_corr_columns_bool]\n",
    "    print(len(selected_columns))\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_split_5(X, y):\n",
    "    \"\"\"\n",
    "    Save a training set and a test set to split a dataframe\n",
    "    :param X: X features\n",
    "    :param y: y features\n",
    "    :return: Xtrain, Xtest, ytrain, ytest\n",
    "    Author: Jaehee Jeong\n",
    "    Date: 03/09/2021\n",
    "    Contact email: jjeong720@ucla.edu\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        Xtrain, Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "    return (Xtrain, Xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one logger for current file, per\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save variables to connent to AWS\n",
    "BUCKET_NAME = \"stats404-project-jaehee\"\n",
    "KEY_NAME_DATA = \"bankrupt_data.csv\"\n",
    "KEY_NAME_MODEL = \"cat_model.joblib\"\n",
    "FILE_NAME = \"s3://stats404-project-jaehee/bankrupt_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 1: Connect to S3 Bucket on AWS\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"Save location of notebook\")\n",
    "    \n",
    "    # Save location of notebook\n",
    "    notebook_dir = os.getcwd()\n",
    "    \n",
    "    # Path to repository on my machine\n",
    "    print(notebook_dir)\n",
    "    \n",
    "    # Change directories to the repository on your local machine:\n",
    "    bankrupt_dir = notebook_dir\n",
    "    os.chdir(bankrupt_dir)\n",
    "    LOGGER.info(\"--- Part 1: Connect to S3 Bucket on AWS\")\n",
    "\n",
    "    # Connect to AWS\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "\n",
    "    # Sse AWS credentials to connect to file system, not as an anonymous user\n",
    "    s3_fs = s3fs.S3FileSystem(anon=False)\n",
    "    # Print bucket name and objects\n",
    "    print_bucket_name_objects()\n",
    "\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 2: Download CSV File from S3 Bucket\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"--- Part 2: Download CSV File from S3 Bucket\")\n",
    "\n",
    "    # Download the dataset in my S3 Bucket and save as df:\n",
    "    LOGGER.info(\"    Download a dataset for bankruptcy\")\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=FILE_NAME, encoding=\"latin-1\", index_col=0\n",
    "    )\n",
    "\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 3: Check dataframe\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"--- Part 3: Check dataframe\")\n",
    "\n",
    "    # Check the shape of df\n",
    "    print(df.shape)\n",
    "\n",
    "    # Check if there are missing values\n",
    "    print(df.info())\n",
    "\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 4: Feature Engineering - Feature Selection\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"--- Part 4: Feature Engineering - Feature Selection\")\n",
    "    LOGGER.info(\"--- 4-1) Step1: Remove space from columns\")\n",
    "\n",
    "    # Remove spaces in the columns\n",
    "    COLUMNS = df.columns.tolist()\n",
    "    NO_SPACE_COLUMNS = [x.strip(\" \") for x in COLUMNS]\n",
    "    df.columns = NO_SPACE_COLUMNS\n",
    "\n",
    "    # Split X features and y features to calculate correlations\n",
    "    X = get_X(df)\n",
    "    y = get_y(df)\n",
    "\n",
    "    # Calculate correlations\n",
    "    importances = X.apply(lambda x: x.corr(y))\n",
    "    # Order by high correlations\n",
    "    index = np.argsort(importances)\n",
    "    print(importances[index])\n",
    "\n",
    "    # Save columns to subtract later\n",
    "    COLUMNS = df.columns[1:]\n",
    "\n",
    "    # Generate a correlation graph to check\n",
    "    correlation_with_bankrupt_graph(importances, index, COLUMNS)\n",
    "\n",
    "    # Save the highly correlated features to get meaningful features\n",
    "    high_corr_columns = save_variables_high_correlation_with_bank(\n",
    "        importances, index, COLUMNS\n",
    "    )\n",
    "    LOGGER.info(\n",
    "        \"Step2: Subtract important X features from all the X features\"\n",
    "    )\n",
    "    # Subtract important X features from all the X features\n",
    "    X = df.loc[:, high_corr_columns]\n",
    "    print(X.shape)\n",
    "    LOGGER.info(\n",
    "        \"Step3: Save the columns that are independent\"\n",
    "    )\n",
    "    # Save the columns that are independent to remove dependent features\n",
    "    selected_columns = remove_dependent_variables(X, high_corr_columns)\n",
    "\n",
    "    # Now we only have 21 features\n",
    "    LOGGER.info(\n",
    "        \"Step4: Save all the selected variables to new_df and Bankrupt\"\n",
    "    )\n",
    "\n",
    "    # Generate a new df with the important features\n",
    "    # Save some features that I think it's important\n",
    "    important_features = [\n",
    "        \"net income to total assets\",\n",
    "        \"Cash flow to total assets\",\n",
    "        \"tax Pre-net interest rate\",\n",
    "        \"inventory and accounts receivable/net value\",\n",
    "    ]\n",
    "\n",
    "    # Save y feature and all important X features\n",
    "    new_df = pd.concat(\n",
    "        [df[\"Bankrupt?\"], \n",
    "        df[important_features], \n",
    "        X[selected_columns]], \n",
    "        axis=1\n",
    "    )\n",
    "    print(new_df.columns)\n",
    "    print(new_df.shape)\n",
    "\n",
    "    # We have 25 features and target.\n",
    "\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 5: Split the dataset\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"--- Part 5: Split the dataset\")\n",
    "    LOGGER.info(\"--- Split the dataset into training and test sets\")\n",
    "\n",
    "    # Split X features and y feature\n",
    "    X = get_X(new_df)\n",
    "    y = get_y(new_df)\n",
    "\n",
    "    # Split the dataset to a training set and a test set\n",
    "    Xtrain, Xtest, ytrain, ytest = Kfold_split_5(X, y)\n",
    "    LOGGER.info(\"--- Export a training set and a test set as csv\")\n",
    "\n",
    "    # Export a training set and a test set\n",
    "    Xtrain.to_csv(os.path.join(bankrupt_dir,r'bankrupt_training_set.csv'),\n",
    "                index=False,\n",
    "                header=True)\n",
    "    Xtest.to_csv(os.path.join(bankrupt_dir,r'bankrupt_test_set.csv'),\n",
    "                index=False,\n",
    "                header=True)\n",
    "\n",
    "    # Check the shape of df\n",
    "    print(Xtrain.shape)\n",
    "    print(Xtest.shape)\n",
    "    LOGGER.info(\"--- Splite the data into training and valiation sets\")\n",
    "    # Split the training dataset to a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        Xtrain, ytrain, \n",
    "        test_size=0.1, \n",
    "        random_state=1, \n",
    "        stratify=ytrain, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 6: Generates models\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"--- Part6: Part 6: Generates models\")\n",
    "    LOGGER.info(\"--- Generate a logistics regression model\")\n",
    "\n",
    "    # Generate a logistic regression model to predict bankrupt\n",
    "    log_reg = LogisticRegression(class_weight=\"balanced\", max_iter=10000)\n",
    "    log_model = log_reg.fit(X_train, y_train)\n",
    "    LOGGER.info(\"--- Print classification and confusion matrix with y_val\")\n",
    "    log_pred = log_model.predict(X_val)\n",
    "    # Generate a classification report and a confusion matrix to check results\n",
    "    print(classification_report(y_val, log_pred))\n",
    "    print_confusion_matrix(y_val, log_pred)\n",
    "\n",
    "    # check a classification & confusion matrix\n",
    "    LOGGER.info(\"--- Print classification and confusion matrix with ytest\")\n",
    "    log_pred = log_model.predict(Xtest)\n",
    "    print(classification_report(ytest, log_pred))\n",
    "    print_confusion_matrix(ytest, log_pred)\n",
    "    LOGGER.info(\"--- Generate a catboost classification model\")\n",
    "\n",
    "    # Define parameters for a catboost model\n",
    "    params = {\n",
    "        \"iterations\": 500,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"depth\": 6,\n",
    "        \"l2_leaf_reg\": 1e-20,\n",
    "        \"eval_metric\": \"Accuracy\",\n",
    "        \"leaf_estimation_iterations\": 10,\n",
    "        \"use_best_model\": True,\n",
    "        \"logging_level\": \"Silent\",\n",
    "        \"random_seed\": 42,\n",
    "        \"class_weights\": (1, 30),\n",
    "    }\n",
    "    cat = CatBoostClassifier(**params)\n",
    "    cat_model = cat.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "    LOGGER.info(\"--- Print classification and confusion matrix with y_val\")\n",
    "    cat_pred = cat_model.predict(X_val)\n",
    "    print(classification_report(y_val, cat_pred))\n",
    "    print_confusion_matrix(y_val, cat_pred)\n",
    "    LOGGER.info(\"--- Print classification and confusion matrix with ytest\")\n",
    "    cat_pred = cat_model.predict(Xtest)\n",
    "    print(classification_report(ytest, cat_pred))\n",
    "    print_confusion_matrix(ytest, cat_pred)\n",
    "    LOGGER.info(\n",
    "        \"--- Generate a precision recall curve to see threshold\"\n",
    "    )\n",
    "    # Generate a precision recall curve to check recall by threshold\n",
    "    generate_precision_recall_chart(Xtest, ytest)\n",
    "    LOGGER.info(\"--- Change threshold and check the result\")\n",
    "    LOGGER.info(\"--- Print classification and confusion matrix with y_val\")\n",
    "    # Change the threshold to 0.37 to improve the prediction\n",
    "    thre_37_preds_cat = cat_model_pred_prob_threshold_37(X_val)\n",
    "    print(classification_report(y_val, thre_37_preds_cat))\n",
    "    print_confusion_matrix(y_val, thre_37_preds_cat)\n",
    "    LOGGER.info(\"--- Print classification and confusion matrix with ytest\")\n",
    "    thre_37_preds_cat = cat_model_pred_prob_threshold_37(Xtest)\n",
    "    print(classification_report(ytest, thre_37_preds_cat))\n",
    "    print_confusion_matrix(ytest, thre_37_preds_cat)\n",
    "\n",
    "    ### -----------------------------------------------------------------------\n",
    "    ### --- Part 7: Upload Model Object to S3 Bucket\n",
    "    ### -----------------------------------------------------------------------\n",
    "    LOGGER.info(\"--- Part 7: Upload Model Object to S3 Bucket\")\n",
    "\n",
    "    # Specify location and name of object to contain estimated model\n",
    "    model_object_path = os.path.join(notebook_dir, \"cat_model.joblib\")\n",
    "    # Save estimated model to specified location\n",
    "    dump(cat_model, model_object_path)\n",
    "    LOGGER.info(\"    Loading Catboost model object\")\n",
    "    cat_model = joblib.load(\"cat_model.joblib\")\n",
    "\n",
    "    # Specify name of file to be created on s3, to store this model object:\n",
    "    upload_model_to_aws(s3_fs, cat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstable = new_df[new_df[\"Bankrupt?\"] == 0]\\nstable\\nstable_mean = stable.mean()\\nstable_dic = stable_mean.to_dict()\\nstable_dic\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "stable = new_df[new_df[\"Bankrupt?\"] == 0]\n",
    "stable\n",
    "stable_mean = stable.mean()\n",
    "stable_dic = stable_mean.to_dict()\n",
    "stable_dic\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
